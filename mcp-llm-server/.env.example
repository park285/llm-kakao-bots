# =============================================================================
# MCP LLM Server Environment Configuration
# =============================================================================
# 이 파일을 .env로 복사하여 사용
# cp .env.example .env

# =============================================================================
# Google Gemini API
# =============================================================================

# API Key (필수)
GOOGLE_API_KEY=your_api_key_here

# Multiple keys for rotation (optional, comma/space separated)
# Example: GOOGLE_API_KEYS="key1,key2 key3"
GOOGLE_API_KEYS=

# =============================================================================
# Model Configuration
# =============================================================================

# Default model for general chat
# Available:
#   - gemini-2.5-flash-preview-09-2025 (2.5 Flash Preview - ThinkingBudget 사용)
#   - gemini-3-pro-preview (3.0 - ThinkingLevel 사용)
#   - gemini-2.5-flash (2.5 Flash Stable)
#   - gemini-2.5-pro (2.5 Pro)
GEMINI_MODEL=gemini-2.5-flash-preview-09-2025

# Task-specific model overrides (leave empty to use GEMINI_MODEL)
GEMINI_HINTS_MODEL=
GEMINI_ANSWER_MODEL=
GEMINI_VERIFY_MODEL=

# =============================================================================
# Gemini 3.0 Thinking Level Configuration
# =============================================================================
# ThinkingLevel: "none", "low", "medium", "high"
# Gemini 3.0 모델에서만 사용 (temperature 1.0 고정)

GEMINI_THINKING_LEVEL=low
GEMINI_THINKING_LEVEL_HINTS=low
GEMINI_THINKING_LEVEL_ANSWER=low
GEMINI_THINKING_LEVEL_VERIFY=low

# =============================================================================
# Gemini 2.5 Thinking Budget Configuration
# =============================================================================
# ThinkingBudget: token count (0 = disabled)
# Gemini 2.5 모델에서만 사용

GEMINI_THINKING_BUDGET=0
GEMINI_THINKING_BUDGET_HINTS=8192
GEMINI_THINKING_BUDGET_ANSWER=4096
GEMINI_THINKING_BUDGET_VERIFY=2048

# =============================================================================
# Generation Parameters
# =============================================================================

# Temperature (0.0 ~ 2.0, Gemini 3.0은 1.0 고정)
GEMINI_TEMPERATURE=0.7

# Max output tokens
GEMINI_MAX_TOKENS=8192

# Max retries for API calls
GEMINI_MAX_RETRIES=3

# Timeout in seconds
GEMINI_TIMEOUT=60

# Model cache size (LRU cache)
GEMINI_MODEL_CACHE_SIZE=20

# =============================================================================
# Session Management
# =============================================================================

# Maximum concurrent sessions
MAX_SESSIONS=50

# Session TTL in minutes (1440 = 24 hours)
SESSION_TTL_MINUTES=1440

# Prompt에 포함할 히스토리 Q/A 페어 수 (0 = 히스토리 미포함)
SESSION_HISTORY_MAX_PAIRS=10

# =============================================================================
# LangGraph Session (Redis Checkpointer)
# =============================================================================

# Redis URL for LangGraph checkpointer
REDIS_URL=redis://localhost:6379

# Enable Redis checkpointer (false = use InMemorySaver)
LANGGRAPH_REDIS_ENABLED=true

# =============================================================================
# Injection Guard
# =============================================================================

# Enable/disable injection guard
GUARD_ENABLED=true

# Detection threshold (0.0 ~ 1.0, higher = more strict)
GUARD_THRESHOLD=0.85

# Rulepacks directory (relative to package)
RULEPACKS_DIR=rulepacks

# Cache settings
GUARD_CACHE_SIZE=10000
GUARD_CACHE_TTL=3600

# Anomaly detection threshold (0.0 ~ 1.0)
GUARD_ANOMALY_THRESHOLD=0.5

# =============================================================================
# MCP Server
# =============================================================================

# Server name
MCP_SERVER_NAME=LLM Server

# =============================================================================
# HTTP Server (REST API + MCP over HTTP)
# =============================================================================

# Host to bind (0.0.0.0 for all interfaces)
HTTP_HOST=127.0.0.1

# Port number (4만번대 사용)
HTTP_PORT=40527

# HTTP/2 (h2c) enable/disable
HTTP2_ENABLED=true

# =============================================================================
# Bot Health Monitor (HTTP only)
# =============================================================================

# Enable/disable bot health watchdog (LLM 서버 내장)
BOT_HEALTH_ENABLED=true

# Health check targets (HTTP)
# - single: BOT_HEALTH_URL
# - multiple: BOT_HEALTH_URLS (comma/space-separated)
BOT_HEALTH_URL=http://127.0.0.1:40527/health/ready
BOT_HEALTH_URLS=

# Restart command and thresholds
# 1) 도커 API로 재시작: BOT_RESTART_CONTAINERS 지정 + BOT_DOCKER_SOCKET 마운트
# 2) 외부 스크립트 호출: BOT_RESTART_CMD에 실행 가능 경로 설정
BOT_RESTART_CONTAINERS=turtle-soup-bot twentyq-bot
BOT_DOCKER_SOCKET=/var/run/docker.sock
BOT_RESTART_CMD=
BOT_HEALTH_INTERVAL_SECONDS=60
BOT_HEALTH_MAX_FAILURES=5
BOT_HEALTH_TIMEOUT_SECONDS=3

# =============================================================================
# PostgreSQL Database (Usage Tracking)
# =============================================================================
# 봇의 token_usage 테이블 공유

DB_HOST=localhost
DB_PORT=5432
DB_NAME=twentyq
DB_USER=twentyq
DB_PASSWORD=your_password_here

# Connection pool settings
DB_MIN_POOL=1
DB_MAX_POOL=5

# =============================================================================
# Logging (loguru)
# =============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Log directory
LOG_DIR=logs

# Log rotation ("10 MB", "1 day", "00:00" 등)
LOG_ROTATION=10 MB

# Log retention ("7 days", "1 month" 등)
LOG_RETENTION=7 days

# Log compression ("gz", "zip", "" for none)
LOG_COMPRESSION=gz

# JSON structured logs (true/false)
LOG_JSON=false

# Console logging (true/false) - 개발/디버깅용
LOG_CONSOLE=false
